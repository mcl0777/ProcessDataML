{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancierung und Erweiterung der Klassifikations Datensätze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstellungen & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import smogn\n",
    "import os\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for plots\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 12\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Test Split \n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade den DataFrame aus der Datei\n",
    "df = pd.read_pickle(\"../datasets/new_features.pkl\")\n",
    "\n",
    "# Überprüfe die Struktur des geladenen DataFrames\n",
    "print(\"Shape des geladenen DataFrames:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(df, target_column, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Führt einen stratifizierten Split basierend auf der Zielspalte durch.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Der Eingabe-DataFrame.\n",
    "        target_column (str): Die Zielspalte für den Stratified Split.\n",
    "        test_size (float): Anteil der Testdaten.\n",
    "        random_state (int): Seed für die Reproduzierbarkeit.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Trainings- und Testdatensätze.\n",
    "    \"\"\"\n",
    "    strat_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_idx, test_idx in strat_split.split(df, df[target_column]):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_and_augment_with_smote(train_df, target_column, target_count_per_class):\n",
    "    \"\"\"\n",
    "    Balanciert den Trainingsdatensatz durch Hinzufügen synthetischer Instanzen\n",
    "    mithilfe von SMOTE, ohne Originaldaten zu reduzieren, und erreicht eine feste Zielgröße pro Klasse.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Trainingsdatensatz.\n",
    "        target_column (str): Die Zielspalte, die balanciert werden soll.\n",
    "        target_count_per_class (int): Zielgröße für jede Klasse.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Der ausgeglichene Trainingsdatensatz mit allen Originaldaten und zusätzlichen synthetischen Daten.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    \n",
    "    # Trennen der Features und der Zielspalte\n",
    "    X = train_df.drop(columns=[target_column, 'VersuchID'], errors='ignore')  # 'VersuchID' nicht für SMOTE verwenden\n",
    "    y = train_df[target_column]\n",
    "\n",
    "    # Speichere die Originaldaten mit Label\n",
    "    original_data = train_df.copy()\n",
    "    original_data['synthetisch'] = False  # Markiere Originaldaten als nicht synthetisch\n",
    "\n",
    "    # SMOTE anwenden\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Kombinieren der Originaldaten und der SMOTE-Daten\n",
    "    smote_generated_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    smote_generated_df[target_column] = y_resampled\n",
    "\n",
    "    # **Fix für korrektes Labeling: Synthetische Daten markieren, aber Originale beibehalten**\n",
    "    smote_generated_df['synthetisch'] = True\n",
    "    smote_generated_df.loc[smote_generated_df.index < len(original_data), 'synthetisch'] = False\n",
    "\n",
    "    # Zielgröße pro Klasse sicherstellen\n",
    "    final_df = [smote_generated_df]  # Füge die vollständige SMOTE-Datenmenge ein\n",
    "    for cls in smote_generated_df[target_column].unique():\n",
    "        cls_data = smote_generated_df[smote_generated_df[target_column] == cls]\n",
    "        if len(cls_data) < target_count_per_class:\n",
    "            # Zusätzliche Instanzen mit SMOTE generieren\n",
    "            additional_data = cls_data.sample(\n",
    "                n=target_count_per_class - len(cls_data),\n",
    "                replace=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            additional_data['synthetisch'] = True  # Markiere zusätzliche Daten als synthetisch\n",
    "            final_df.append(additional_data)\n",
    "\n",
    "    return pd.concat(final_df, ignore_index=True)\n",
    "\n",
    "def process_targets_with_smote(df, target_columns, test_size=0.2, target_count_per_class=500):\n",
    "    \"\"\"\n",
    "    Splittet die Daten und balanciert jede Zielspalte mithilfe von SMOTE,\n",
    "    ohne Originaldaten zu entfernen.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Der Eingabe-DataFrame.\n",
    "        target_columns (list): Liste der Zielspalten.\n",
    "        test_size (float): Anteil der Testdaten.\n",
    "        target_count_per_class (int): Anzahl der Instanzen pro Klasse nach Balancing.\n",
    "\n",
    "    Returns:\n",
    "        dict: Trainings- und Testdatensätze für jede Zielspalte.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "\n",
    "    for target in target_columns:\n",
    "        print(f\"\\n>>> Verarbeitung für Zielgröße: {target} <<<\")\n",
    "        \n",
    "        # Stratified Split\n",
    "        train_df, test_df = stratified_split(df, target, test_size=test_size)\n",
    "\n",
    "        print(f\"Verteilung vor Balancing im Trainingsdatensatz für '{target}':\")\n",
    "        print(train_df[target].value_counts())\n",
    "\n",
    "        # Balancing mit SMOTE ohne Entfernen von Originaldaten\n",
    "        train_balanced_df = balance_and_augment_with_smote(train_df, target, target_count_per_class)\n",
    "\n",
    "        print(f\"Verteilung nach Balancing im Trainingsdatensatz für '{target}':\")\n",
    "        print(train_balanced_df[target].value_counts())\n",
    "\n",
    "        # Speichern der Ergebnisse\n",
    "        datasets[target] = {\n",
    "            \"train\": train_balanced_df,\n",
    "            \"test\": test_df\n",
    "        }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE für Klassifikations-Zielgrößen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zielspalten (Anpassen an Ihren Datensatz)\n",
    "target_columns = ['Ergebnis_con', 'Material_con', 'Position_con', 'richtig_verbaut']\n",
    "target_count = 2500\n",
    "# Verarbeitung der Zielgrößen\n",
    "if test_size == 0.3:\n",
    "        datasets = process_targets_with_smote(df, target_columns, test_size=0.3, target_count_per_class=target_count)\n",
    "else:\n",
    "    datasets = process_targets_with_smote(df, target_columns, test_size=0.2, target_count_per_class=target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Speichern der Ergebnisse\n",
    "# for target, data in datasets.items():\n",
    "    \n",
    "#     if test_size == 0.3:\n",
    "#         train_path = f\"../datasets/train_balanced_{target}_smote_testsize03_{target_count}.pkl\"\n",
    "#         test_path = f\"../datasets/test_{target}_30.pkl\"\n",
    "#     else:\n",
    "#         train_path = f\"../datasets/train_balanced_{target}_smote_{target_count}.pkl\"\n",
    "#         test_path = f\"../datasets/test_{target}.pkl\"\n",
    "    \n",
    "    \n",
    "#     data[\"train\"].to_pickle(train_path)\n",
    "#     data[\"test\"].to_pickle(test_path)\n",
    "\n",
    "#     print(f\"Trainings- und Testdatensätze für '{target}' gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary der Datensätze für Zielgrößen\n",
    "datasets_classification = {\n",
    "    \"Ergebnis_con\": {\n",
    "        \"train\": f\"../Datasets/train_balanced_Ergebnis_con_smote_{target_count}.pkl\",\n",
    "        \"test\": \"../Datasets/test_Ergebnis_con.pkl\"\n",
    "    },\n",
    "    \"Material_con\": {\n",
    "        \"train\": f\"../Datasets/train_balanced_Material_con_smote_{target_count}.pkl\",\n",
    "        \"test\": \"../Datasets/test_Material_con.pkl\"\n",
    "    },\n",
    "    \"Position_con\": {\n",
    "        \"train\": f\"../Datasets/train_balanced_Position_con_smote_{target_count}.pkl\",\n",
    "        \"test\": \"../Datasets/test_Position_con.pkl\"\n",
    "    },\n",
    "    \"richtig_verbaut\": {\n",
    "        \"train\": f\"../Datasets/train_balanced_richtig_verbaut_smote_{target_count}.pkl\",\n",
    "        \"test\": \"../Datasets/test_richtig_verbaut.pkl\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairplot(data, target_column, title, ignored_features=None):\n",
    "    \"\"\"\n",
    "    Erstellt einen Pairplot für einen gegebenen DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Der Eingabedatensatz.\n",
    "        target_column (str): Die Zielspalte.\n",
    "        title (str): Der Titel des Plots.\n",
    "        ignored_features (list, optional): Liste der zu ignorierenden Features. Standard ist None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Datensatzgröße: {data.shape}\")\n",
    "    print(data[target_column].value_counts())\n",
    "\n",
    "    # Ignoriere ausgewählte Features\n",
    "    if ignored_features:\n",
    "        data = data.drop(columns=ignored_features, errors='ignore')\n",
    "    \n",
    "    # Wähle numerische Spalten und Zielspalte\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int']).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col != target_column]\n",
    "    \n",
    "    if len(numeric_cols) > 5:  # Maximal 5 Spalten für Übersichtlichkeit\n",
    "        numeric_cols = numeric_cols[:5]\n",
    "    \n",
    "    # Erstelle den Pairplot und unterscheide Klassen durch Farben\n",
    "    g = sns.pairplot(\n",
    "        data[numeric_cols + [target_column]],\n",
    "        hue=target_column,\n",
    "        diag_kind=\"kde\",\n",
    "        height=2.5,  # Erhöht die Größe der Plots\n",
    "        plot_kws={'alpha': 0.6, 's': 20},  # Transparenz und Punktgröße anpassen\n",
    "        markers=[\"o\", \"s\"],  # Markierung für die Klassen\n",
    "    )\n",
    "    g.fig.suptitle(title, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der zu ignorierenden Features\n",
    "ignored_features = []  # Beispiel: ['Feature1', 'Feature2']\n",
    "\n",
    "# Zielspalten für jede Zielgröße\n",
    "target_columns = {\n",
    "    \"Ergebnis_con\": \"Ergebnis_con\",\n",
    "    \"Material_con\": \"Material_con\",\n",
    "    \"Position_con\": \"Position_con\",\n",
    "    \"richtig_verbaut\": \"richtig_verbaut\",\n",
    "    #\"Probenhoehe\": \"Probenhoehe\"\n",
    "}\n",
    "\n",
    "# Generiere Pairplots und Klassenverteilungen für die Trainingsdatensätze\n",
    "for target, paths in datasets_classification.items():\n",
    "    print(f\"Pairplot für Trainingsdatensatz: {target}\")\n",
    "    # Laden des DataFrames\n",
    "    train_data = pd.read_pickle(paths[\"train\"])\n",
    "    create_pairplot(train_data, target_columns[target], f\"Train: {target}\", ignored_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real vs. Synthetisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datei laden\n",
    "zielgroeße = \"Position_con\"\n",
    "df_train = pd.read_pickle(f\"../datasets/train_balanced_{zielgroeße}_smote_2500.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.groupby(['synthetisch', zielgroeße]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: Die Features sind alle Spalten außer der Zielvariable & kategorischen Spalten\n",
    "features = [col for col in df_train.columns if col not in ['Ergebnis_con', 'VersuchID', 'Material_con', 'Position_con', 'richtig_verbaut','Probenhoehe']]\n",
    "\n",
    "\n",
    "# # Boxplots getrennt nach Ergebnis_con (0 vs. 1)\n",
    "# for feature in features:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='synthetisch', y=feature, hue='Ergebnis_con', data=df_train, palette=\"Set2\")\n",
    "    \n",
    "#     plt.title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "#     plt.xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.legend(title=\"Ergebnis_con\", loc=\"upper right\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# Setze die Features für die Subplots\n",
    "features_to_plot = [\"Berührzeit\", \"Motorstrom_Durchschnitt\"]\n",
    "\n",
    "# Erstelle die Subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # 1 Zeile, 2 Spalten\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    sns.boxplot(x='synthetisch', y=feature, hue=zielgroeße, data=df_train, palette=\"Set2\", ax=axes[i])\n",
    "    axes[i].set_title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "    axes[i].set_xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "    axes[i].set_ylabel(feature)\n",
    "    axes[i].legend(title=zielgroeße, loc=\"upper right\")\n",
    "\n",
    "# Subplots platzsparend anordnen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erweiterung Probenhöhe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Spezifische Anwendung für Probenhöhe**\n",
    "target_column = \"Probenhoehe\"\n",
    "\n",
    "# Datensatz laden\n",
    "df = pd.read_pickle(\"../datasets/new_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_visualize_outliers_by_group(df, column_name, group_col):\n",
    "    \"\"\"\n",
    "    Identifiziert und visualisiert Outlier in 'column_name' getrennt nach 'group_col'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame mit den Daten.\n",
    "        column_name (str): Name der Spalte, in der Outlier identifiziert werden sollen.\n",
    "        group_col (str): Name der Spalte, nach der gruppiert werden soll (z.B. 'NominalProbenhoehe').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mit allen IQR-Outliern aus allen Gruppen.\n",
    "        pd.DataFrame: DataFrame mit allen Z-Score-Outliern aus allen Gruppen.\n",
    "    \"\"\"\n",
    "    # Listen zum Sammeln aller Outlier pro Gruppe\n",
    "    outliers_iqr_list = []\n",
    "    outliers_zscore_list = []\n",
    "\n",
    "    # Gruppieren nach der gewünschten Spalte (z.B. nominale Probenhöhe)\n",
    "    grouped = df.groupby(group_col)\n",
    "\n",
    "    for group_value, group_df in grouped:\n",
    "        print(f\"\\n--- Auswertung für {group_col} = {group_value} ---\")\n",
    "\n",
    "        # 1) IQR-Methode\n",
    "        Q1 = group_df[column_name].quantile(0.25)\n",
    "        Q3 = group_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Grenzen anpassen, falls gewünscht (hier 1.2*IQR wie in deinem Code)\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers_iqr = group_df[\n",
    "            (group_df[column_name] < lower_bound) | (group_df[column_name] > upper_bound)\n",
    "        ]\n",
    "        outliers_iqr_list.append(outliers_iqr)\n",
    "\n",
    "        print(f\"  Anzahl Outlier (IQR) für Gruppe {group_value}: {len(outliers_iqr)}\")\n",
    "\n",
    "        # 2) Z-Score-Methode\n",
    "        # Wir berechnen den Z-Score nur auf die Teilgruppe\n",
    "        group_df = group_df.copy()  # damit wir den Original-DataFrame nicht überschreiben\n",
    "        group_df['Z-Score'] = zscore(group_df[column_name])\n",
    "        outliers_z = group_df[np.abs(group_df['Z-Score']) > 3]\n",
    "        outliers_zscore_list.append(outliers_z)\n",
    "        \n",
    "        print(f\"  Anzahl Outlier (Z-Score) für Gruppe {group_value}: {len(outliers_z)}\")\n",
    "\n",
    "        # -- Visualisierung pro Gruppe (optional) --\n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=group_df[column_name])\n",
    "        plt.title(f\"Boxplot der {column_name} (Gruppe: {group_value})\")\n",
    "        plt.show()\n",
    "\n",
    "        # Histogramm\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.histplot(group_df[column_name], bins=30, kde=True, color=\"blue\")\n",
    "        plt.axvline(lower_bound, color=\"red\", linestyle=\"--\", label=\"IQR Untergrenze\")\n",
    "        plt.axvline(upper_bound, color=\"red\", linestyle=\"--\", label=\"IQR Obergrenze\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Histogramm der {column_name} (Gruppe: {group_value})\")\n",
    "        plt.show()\n",
    "\n",
    "    # Zusammenführen aller Outlier aus allen Gruppen\n",
    "    outliers_iqr_all = pd.concat(outliers_iqr_list) if outliers_iqr_list else pd.DataFrame()\n",
    "    outliers_zscore_all = pd.concat(outliers_zscore_list) if outliers_zscore_list else pd.DataFrame()\n",
    "\n",
    "    return outliers_iqr_all, outliers_zscore_all\n",
    "\n",
    "def remove_outliers_by_group(df, column_name, group_col):\n",
    "    \"\"\"\n",
    "    Entfernt Outlier aus dem DataFrame, gruppiert nach einer nominalen Spalte.\n",
    "    Die Outlier werden mithilfe der IQR-Methode innerhalb jeder Gruppe identifiziert.\n",
    "    Anschließend wird der DataFrame bereinigt und die Gruppierungsspalte entfernt.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Ursprünglicher DataFrame.\n",
    "        column_name (str): Name der Spalte, in der Outlier erkannt werden sollen.\n",
    "        group_col (str): Name der Spalte, die die nominalen Werte (z.B. 38, 40, 42) enthält.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Bereinigter DataFrame ohne Outlier und ohne die Gruppierungsspalte.\n",
    "    \"\"\"\n",
    "    # Kopie erstellen, um das Original beizubehalten\n",
    "    df_clean = df.copy()\n",
    "    outlier_indices = []\n",
    "\n",
    "    # Gruppierung nach der nominalen Probenhöhe (die zuvor aus den IST-Werten erstellt wurde)\n",
    "    for group_value, group_df in df_clean.groupby(group_col):\n",
    "        Q1 = group_df[column_name].quantile(0.25)\n",
    "        Q3 = group_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Grenzen der IQR-Methode (hier 1.2 * IQR, kann angepasst werden)\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identifikation der Outlier in dieser Gruppe\n",
    "        group_outliers = group_df[(group_df[column_name] < lower_bound) | (group_df[column_name] > upper_bound)]\n",
    "        outlier_indices.extend(group_outliers.index.tolist())\n",
    "\n",
    "    # Doppelte Indizes entfernen\n",
    "    outlier_indices = list(set(outlier_indices))\n",
    "    \n",
    "    # Entferne die Zeilen mit Outliern aus dem DataFrame\n",
    "    df_clean = df_clean.drop(index=outlier_indices)\n",
    "    \n",
    "    # Entferne die nominale Spalte, die zur Gruppierung genutzt wurde\n",
    "    df_clean = df_clean.drop(columns=[group_col])\n",
    "    \n",
    "    return df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die nominalen Werte\n",
    "nominal_values = np.array([38, 40, 42])\n",
    "\n",
    "# Funktion, um den nächstgelegenen nominalen Wert zu bestimmen\n",
    "def assign_nominal_value(ist_value, nominal_values=nominal_values):\n",
    "    distances = np.abs(nominal_values - ist_value)\n",
    "    return nominal_values[np.argmin(distances)]\n",
    "\n",
    "# Hilfssplte für Darstellung\n",
    "df['NominalProbenhoehe'] = df['Probenhoehe'].apply(assign_nominal_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Originale Daten:\")\n",
    "outlier_df1 = detect_and_visualize_outliers_by_group(df, \"Probenhoehe\",group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame ohne Outlier\n",
    "df_clean = remove_outliers_by_group(df, column_name=\"Probenhoehe\", group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung bereinigter original Daten\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_clean['Probenhoehe'], bins=300, kde=True, color=\"#4C72B0\")\n",
    "plt.title(\"Verteilung der Probenhöhe im bereinigten originalen Datensatz\")\n",
    "plt.xlabel(\"Probenhöhe (mm)\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in Training und Test\n",
    "train_df, test_df = train_test_split(df_clean, test_size=test_size, random_state=42)\n",
    "train_df.to_pickle(f\"../datasets/train_{target_column}_clean.pkl\")\n",
    "test_df.to_pickle(f\"../datasets/test_{target_column}_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smogn_with_inverse_scaling(df, target_col, mode_ranges, samples_per_mode=500, k=5, rel_thres=0.8, random_state=42, use_manual_relevance=False):\n",
    "    \"\"\"\n",
    "    Erzeugt synthetische Daten mit SMOGN und führt anschließend eine inverse Skalierung und Rücktransformation der label-encodeten Spalten durch.\n",
    "    \n",
    "    Parameter:\n",
    "      df: Ursprünglicher DataFrame.\n",
    "      target_col: Name der Zielspalte.\n",
    "      mode_ranges: Liste von Tupeln, die die Bereiche definieren (min, max) für die SMOGN-Anwendung.\n",
    "      samples_per_mode: Maximale Anzahl synthetischer Samples pro Bereich.\n",
    "      k: Parameter für SMOGN (Anzahl der Nachbarn).\n",
    "      rel_thres: Schwellenwert für Relevanz.\n",
    "      random_state: Zufallsseed.\n",
    "      use_manual_relevance: Falls True, wird die manuelle Relevanzfunktion verwendet (mit definierten Kontrollpunkten),\n",
    "                              andernfalls wird SMOGNs automatischer Relevanzmechanismus genutzt.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # --- Kategorische Spalten erkennen und label-encoden ---\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    label_encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # --- Fehlende Werte behandeln ---\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[df.columns] = imputer.fit_transform(df)\n",
    "\n",
    "    # --- Skalierung (nur Features) ---\n",
    "    scaler = StandardScaler()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    df_scaled = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # --- Relevanzparameter definieren ---\n",
    "    if use_manual_relevance:\n",
    "        rel_method = \"manual\"\n",
    "        rel_ctrl_pts_rg = [\n",
    "            [35.0, 0.0],\n",
    "            [38.0, 1.0],\n",
    "            [38.5, 0.0],\n",
    "            [39.5, 0.0],\n",
    "            [40.0, 1.0],\n",
    "            [40.5, 0.0],\n",
    "            [41.5, 0.0],\n",
    "            [42.0, 1.0],\n",
    "            [42.5, 0.0],\n",
    "            [45.0, 0.0]\n",
    "        ]\n",
    "    else:\n",
    "        rel_method = \"auto\"\n",
    "        rel_ctrl_pts_rg = None  # wird bei \"auto\" nicht benötigt\n",
    "\n",
    "    all_augmented = []\n",
    "\n",
    "    for min_val, max_val in mode_ranges:\n",
    "        try:\n",
    "            # Auswahl des Teilbereichs basierend auf dem Zielwert\n",
    "            subset = df_scaled[(df_scaled[target_col] >= min_val) & (df_scaled[target_col] <= max_val)].copy()\n",
    "            if subset.empty or subset.shape[0] < k + 1:\n",
    "                print(f\"Zu wenige Daten im Bereich {min_val}-{max_val}, überspringe...\")\n",
    "                continue\n",
    "\n",
    "            subset.reset_index(drop=True, inplace=True)\n",
    "            original_columns = subset.columns.tolist()\n",
    "            print(f\"[DEBUG] Bereich {min_val}-{max_val}: subset.shape = {subset.shape}\")\n",
    "\n",
    "            # --- SMOGN-Anwendung ---\n",
    "            if use_manual_relevance:\n",
    "                smogn_result = smogn.smoter(\n",
    "                    data=subset.copy(),\n",
    "                    y=target_col,\n",
    "                    samp_method=\"balance\",\n",
    "                    rel_method=rel_method,\n",
    "                    rel_ctrl_pts_rg=rel_ctrl_pts_rg,\n",
    "                    rel_thres=rel_thres,\n",
    "                    k=k\n",
    "                )\n",
    "            else:\n",
    "                smogn_result = smogn.smoter(\n",
    "                    data=subset.copy(),\n",
    "                    y=target_col,\n",
    "                    samp_method=\"balance\",\n",
    "                    rel_method=rel_method,\n",
    "                    rel_thres=rel_thres,\n",
    "                    k=k\n",
    "                )\n",
    "\n",
    "            print(f\"[DEBUG] SMOGN-Ergebnis vor Spaltenanpassung: shape = {smogn_result.shape}, columns = {list(smogn_result.columns)}\")\n",
    "\n",
    "            # Entferne evtl. vorhandene zusätzliche Spalte(n), z. B. \"rel\"\n",
    "            extra_cols = [col for col in smogn_result.columns if col not in original_columns]\n",
    "            if extra_cols:\n",
    "                smogn_result.drop(columns=extra_cols, inplace=True)\n",
    "                print(f\"[DEBUG] Entfernte Spalten: {extra_cols}\")\n",
    "\n",
    "            # Prüfe, ob die Spaltenanzahl übereinstimmt\n",
    "            if smogn_result.shape[1] != subset.shape[1]:\n",
    "                print(f\"Spaltenanzahl stimmt nicht überein nach SMOGN für Bereich {min_val}-{max_val}. Erhalten: {smogn_result.shape[1]}, erwartet: {subset.shape[1]}\")\n",
    "                continue\n",
    "\n",
    "            # --- Extraktion der synthetisch generierten Zeilen ---\n",
    "            new_rows = smogn_result.iloc[subset.shape[0]:].copy()\n",
    "            print(f\"[DEBUG] Für Bereich {min_val}-{max_val}: new_rows.shape = {new_rows.shape}\")\n",
    "            if new_rows.empty:\n",
    "                print(f\"Keine neuen Daten erzeugt für Bereich {min_val}-{max_val}\")\n",
    "                continue\n",
    "\n",
    "            if new_rows.shape[0] > samples_per_mode:\n",
    "                new_rows = new_rows.sample(n=samples_per_mode, random_state=random_state)\n",
    "\n",
    "            new_rows[\"synthetisch\"] = 1\n",
    "            all_augmented.append(new_rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei SMOGN für Bereich {min_val}-{max_val}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_augmented:\n",
    "        raise ValueError(\"Keine synthetischen Daten erzeugt.\")\n",
    "\n",
    "    # --- Rücktransformation der skalierten Features ---\n",
    "    df_augmented_scaled = pd.concat(all_augmented, ignore_index=True)\n",
    "    X_new = df_augmented_scaled.drop(columns=[target_col, \"synthetisch\"])\n",
    "    X_new_inverse = pd.DataFrame(scaler.inverse_transform(X_new), columns=X.columns)\n",
    "    df_augmented = pd.concat([\n",
    "        X_new_inverse,\n",
    "        df_augmented_scaled[[target_col, \"synthetisch\"]].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "\n",
    "    # --- Rücktransformation der label-encodeten kategorialen Variablen ---\n",
    "    for col in cat_cols:\n",
    "        le = label_encoders[col]\n",
    "        df_augmented[col] = df_augmented[col].round().astype(int).clip(0, len(le.classes_) - 1)\n",
    "        df_augmented[col] = le.inverse_transform(df_augmented[col])\n",
    "\n",
    "    # --- Originaldaten (synthetisch = 0) hinzufügen ---\n",
    "    df_original = df.copy()\n",
    "    df_original[\"synthetisch\"] = 0\n",
    "\n",
    "    df_final = pd.concat([df_original, df_augmented], ignore_index=True)\n",
    "    return df_final\n",
    "\n",
    "def compute_phi_relevance(y, rel_thres=0.8):\n",
    "    \"\"\"\n",
    "    Erzeugt Relevanzwerte wie in SMOGN bei rel_method=\"auto\".\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    q1 = np.percentile(y, 25)\n",
    "    q3 = np.percentile(y, 75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    rel = np.where((y < lower_bound) | (y > upper_bound), 1, 0)\n",
    "    return rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_columns = ['Material_con', 'Position_con', 'Ergebnis_con', 'richtig_verbaut', 'Zeit', 'VersuchID','umformzeit' # Umformzeit wurde doppelt berechnet\n",
    "                      ]\n",
    "df_reduced = df_clean.drop(columns=irrelevant_columns, errors='ignore')\n",
    "\n",
    "mode_ranges = [(37.5, 38.5), (39, 41), (41.5, 42.5)]  # Bereich in dem die originalen Daten liegen\n",
    "df_augmented = apply_smogn_with_inverse_scaling(\n",
    "    df_reduced,\n",
    "    target_col=\"Probenhoehe\",\n",
    "    mode_ranges=mode_ranges,\n",
    "    samples_per_mode=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung synthetischer Daten\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_augmented['Probenhoehe'], bins=300, kde=True, color=\"#4C72B0\")\n",
    "plt.title(\"Verteilung der Probenhöhe im augmentierten Datensatz\")\n",
    "plt.xlabel(\"Probenhöhe (mm)\")\n",
    "plt.ylabel(\"Häufigkeit\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfssplte für Visualisierung \n",
    "df_augmented['NominalProbenhoehe'] = df_augmented['Probenhoehe'].apply(assign_nominal_value)\n",
    "\n",
    "outlier_df_test_jittered = detect_and_visualize_outliers_by_group(df_augmented, \"Probenhoehe\",group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erneutes Entfernen der Outlier, falls erforderlich\n",
    "clean_train_df = remove_outliers_by_group(df_augmented, column_name=\"Probenhoehe\", group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung Verteilung der Daten: original vs synthetisch\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.histplot(df_clean['Probenhoehe'], bins=300, kde=True, color=\"#4C72B0\", ax=axs[0])\n",
    "axs[0].set_title(\"Verteilung der Probenhöhe im bereinigten originalen Datensatz\")\n",
    "axs[0].set_xlabel(\"Probenhöhe (mm)\")\n",
    "axs[0].set_ylabel(\"Häufigkeit\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "sns.histplot(df_augmented['Probenhoehe'], bins=300, kde=True, color=\"#4C72B0\", ax=axs[1])\n",
    "axs[1].set_title(\"Verteilung der Probenhöhe im augmentierten Datensatz\")\n",
    "axs[1].set_xlabel(\"Probenhöhe (mm)\")\n",
    "axs[1].set_ylabel(\"Häufigkeit\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"ProbenhöheVerteilungSMOGN.svg\", format=\"svg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfssplte für Visualisierung \n",
    "clean_train_df['NominalProbenhoehe'] = clean_train_df['Probenhoehe'].apply(assign_nominal_value)\n",
    "\n",
    "outlier_df_test_jittered = detect_and_visualize_outliers_by_group(clean_train_df, \"Probenhoehe\",group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen NominalProbenhoehe\n",
    "clean_train_df = clean_train_df.drop(columns=['NominalProbenhoehe',], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: Die Features sind alle Spalten außer der Zielvariable & kategorischen Spalten\n",
    "features = [col for col in clean_train_df.columns if col not in ['Ergebnis_con', 'VersuchID', 'Material_con', 'Position_con', 'richtig_verbaut','Probenhoehe']]\n",
    "\n",
    "\n",
    "# Boxplots getrennt nach Ergebnis_con (0 vs. 1)\n",
    "# for feature in features:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='synthetisch', y=feature, data=clean_train_df, palette=\"Set2\")\n",
    "    \n",
    "#     plt.title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "#     plt.xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.legend(title=\"Probenhoehe\", loc=\"upper right\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# Setze die Features für die Subplots\n",
    "features_to_plot = [\"Berührzeit\", \"Verkippung_2_Min\"]\n",
    "\n",
    "# Erstelle die Subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # 1 Zeile, 2 Spalten\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    sns.boxplot(x='synthetisch', y=feature, data=clean_train_df, palette=\"Set2\", ax=axes[i])\n",
    "    axes[i].set_title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "    axes[i].set_xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Subplots platzsparend anordnen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen der Hilfsspalten \n",
    "df_train = clean_train_df.drop(columns=['synthetisch','Probenhoehe_Gruppe'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der Trainingsdaten\n",
    "#df_train.to_pickle(f\"../datasets/train_{target_column}_smogn.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_regression_target_with_jitter(df, target_column, new_sample_count, jitter_std=0.01):\n",
    "    \"\"\"\n",
    "    Erweiterung der Trainingsdaten für ein Regressionsziel durch Jittering.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Eingabedatensatz.\n",
    "        target_column (str): Die Zielspalte (Regression).\n",
    "        new_sample_count (int): Anzahl der zu generierenden neuen Samples.\n",
    "        jitter_std (float): Standardabweichung für das Hinzufügen von Jitter.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Erweiterter Datensatz.\n",
    "    \"\"\"\n",
    "    print(f\"Originale Datenpunkte: {len(df)}\")\n",
    "    \n",
    "    # Erzeuge neue Samples durch Jittering\n",
    "    jittered_samples = []\n",
    "    for _ in range(new_sample_count):\n",
    "        # Zufällige Auswahl eines existierenden Samples\n",
    "        sample = df.sample(n=1, random_state=np.random.randint(1000))\n",
    "        jittered_sample = sample.copy()\n",
    "        \n",
    "        # Füge Jitter (zufällige Störungen) zu allen numerischen Features hinzu\n",
    "        for col in df.columns:\n",
    "            if col != target_column and np.issubdtype(df[col].dtype, np.number):\n",
    "                jittered_sample[col] += np.random.normal(loc=0.0, scale=jitter_std)\n",
    "        \n",
    "        jittered_samples.append(jittered_sample)\n",
    "    \n",
    "    # Kombiniere Original- und Jittered-Daten\n",
    "    extended_df = pd.concat([df] + jittered_samples, ignore_index=False)\n",
    "    print(f\"Erweiterte Datenpunkte: {len(extended_df)}\")\n",
    "    return extended_df\n",
    "\n",
    "def detect_and_visualize_outliers(df, column_name):\n",
    "    \"\"\"\n",
    "    Identifiziert und visualisiert Outlier in einer bestimmten Spalte eines DataFrames.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame mit den Daten.\n",
    "        column_name (str): Name der Spalte, in der Outlier identifiziert werden sollen.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mit den Outlier-Werten.\n",
    "    \"\"\"\n",
    "    # Berechnung des IQR (Interquartilsabstand)\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.2 * IQR\n",
    "    upper_bound = Q3 + 1.2 * IQR\n",
    "\n",
    "    # Identifikation der Outlier\n",
    "    outliers_iqr = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
    "\n",
    "    # Berechnung des Z-Scores\n",
    "    df['Z-Score'] = zscore(df[column_name])\n",
    "    outliers_zscore = df[np.abs(df['Z-Score']) > 3]\n",
    "\n",
    "    print(f\"Anzahl der Outlier mit IQR-Methode: {len(outliers_iqr)}\")\n",
    "    print(f\"Anzahl der Outlier mit Z-Score-Methode: {len(outliers_zscore)}\")\n",
    "\n",
    "    # Boxplot zur Visualisierung der Outlier\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.boxplot(x=df[column_name])\n",
    "    plt.title(f\"Boxplot der {column_name} mit Outliern\")\n",
    "    plt.show()\n",
    "\n",
    "    # Histogramm der Verteilung\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.histplot(df[column_name], bins=30, kde=True, color=\"blue\")\n",
    "    plt.axvline(lower_bound, color=\"red\", linestyle=\"--\", label=\"IQR Untergrenze\")\n",
    "    plt.axvline(upper_bound, color=\"red\", linestyle=\"--\", label=\"IQR Obergrenze\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Histogramm der {column_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Rückgabe der Outlier\n",
    "    return outliers_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erweiterung der Trainingsdaten\n",
    "new_sample_count = 4540  # Anzahl der zu generierenden zusätzlichen Samples\n",
    "extended_train_df = extend_regression_target_with_jitter(train_df, target_column, new_sample_count)\n",
    "\n",
    "if test_size == 0.3:\n",
    "        train_path = f\"../Datasets/train_{target_column}_jitter_30_5000.pkl\"\n",
    "        test_path = f\"../Datasets/test_{target_column}_30.pkl\"\n",
    "else:\n",
    "        train_path = f\"../Datasets/train_{target_column}_extended_jitter_5000.pkl\"\n",
    "        test_path = f\"../Datasets/test_{target_column}_extended.pkl\"\n",
    "# Speichern der neuen Daten\n",
    "extended_train_df.to_pickle(train_path)\n",
    "test_df.to_pickle(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testset jittern\n",
    "test_df_jittered = extend_regression_target_with_jitter(test_df, target_column, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die nominalen Werte\n",
    "nominal_values = np.array([38, 40, 42])\n",
    "\n",
    "# Funktion, um den nächstgelegenen nominalen Wert zu bestimmen\n",
    "def assign_nominal_value(ist_value, nominal_values=nominal_values):\n",
    "    distances = np.abs(nominal_values - ist_value)\n",
    "    return nominal_values[np.argmin(distances)]\n",
    "\n",
    "# Hilfssplte \n",
    "test_df_jittered['NominalProbenhoehe'] = test_df_jittered['Probenhoehe'].apply(assign_nominal_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df_test_jittered = detect_and_visualize_outliers_by_group(test_df_jittered, \"Probenhoehe\",group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen der Outlier, falls erforderlich\n",
    "clean_test_df = remove_outliers_by_group(test_df_jittered, column_name=\"Probenhoehe\", group_col=\"NominalProbenhoehe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speichern des Testsets\n",
    "clean_test_df.to_pickle(f\"../Datasets/test_{target_column}_jittered.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung auf die Spalte \"Probenhoehe\"\n",
    "print(\"Erweiterte Daten:\")\n",
    "outlier_df = detect_and_visualize_outliers(extended_train_df, \"Probenhoehe\")\n",
    "\n",
    "# Die Outlier anzeigen\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erweiterung Bauteil Temperatur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_visualize_outliers_by_group(df, column_name, group_col):\n",
    "    \"\"\"\n",
    "    Identifiziert und visualisiert Outlier in 'column_name' getrennt nach 'group_col'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame mit den Daten.\n",
    "        column_name (str): Name der Spalte, in der Outlier identifiziert werden sollen.\n",
    "        group_col (str): Name der Spalte, nach der gruppiert werden soll (z.B. 'NominalProbenhoehe').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame mit allen IQR-Outliern aus allen Gruppen.\n",
    "        pd.DataFrame: DataFrame mit allen Z-Score-Outliern aus allen Gruppen.\n",
    "    \"\"\"\n",
    "    # Listen zum Sammeln aller Outlier pro Gruppe\n",
    "    outliers_iqr_list = []\n",
    "    outliers_zscore_list = []\n",
    "\n",
    "    # Gruppieren nach der gewünschten Spalte (z.B. nominale Probenhöhe)\n",
    "    grouped = df.groupby(group_col)\n",
    "\n",
    "    for group_value, group_df in grouped:\n",
    "        print(f\"\\n--- Auswertung für {group_col} = {group_value} ---\")\n",
    "\n",
    "        # 1) IQR-Methode\n",
    "        Q1 = group_df[column_name].quantile(0.25)\n",
    "        Q3 = group_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Grenzen anpassen, falls gewünscht (hier 1.2*IQR wie in deinem Code)\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers_iqr = group_df[\n",
    "            (group_df[column_name] < lower_bound) | (group_df[column_name] > upper_bound)\n",
    "        ]\n",
    "        outliers_iqr_list.append(outliers_iqr)\n",
    "\n",
    "        print(f\"  Anzahl Outlier (IQR) für Gruppe {group_value}: {len(outliers_iqr)}\")\n",
    "\n",
    "        # 2) Z-Score-Methode\n",
    "        # Wir berechnen den Z-Score nur auf die Teilgruppe\n",
    "        group_df = group_df.copy()  # damit wir den Original-DataFrame nicht überschreiben\n",
    "        group_df['Z-Score'] = zscore(group_df[column_name])\n",
    "        outliers_z = group_df[np.abs(group_df['Z-Score']) > 3]\n",
    "        outliers_zscore_list.append(outliers_z)\n",
    "        \n",
    "        print(f\"  Anzahl Outlier (Z-Score) für Gruppe {group_value}: {len(outliers_z)}\")\n",
    "\n",
    "        # -- Visualisierung pro Gruppe (optional) --\n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=group_df[column_name])\n",
    "        plt.title(f\"Boxplot der {column_name} (Gruppe: {group_value})\")\n",
    "        plt.show()\n",
    "\n",
    "        # Histogramm\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.histplot(group_df[column_name], bins=30, kde=True, color=\"blue\")\n",
    "        plt.axvline(lower_bound, color=\"red\", linestyle=\"--\", label=\"IQR Untergrenze\")\n",
    "        plt.axvline(upper_bound, color=\"red\", linestyle=\"--\", label=\"IQR Obergrenze\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Histogramm der {column_name} (Gruppe: {group_value})\")\n",
    "        plt.show()\n",
    "\n",
    "    # Zusammenführen aller Outlier aus allen Gruppen\n",
    "    outliers_iqr_all = pd.concat(outliers_iqr_list) if outliers_iqr_list else pd.DataFrame()\n",
    "    outliers_zscore_all = pd.concat(outliers_zscore_list) if outliers_zscore_list else pd.DataFrame()\n",
    "\n",
    "    return outliers_iqr_all, outliers_zscore_all\n",
    "\n",
    "def remove_outliers_by_group(df, column_name, group_col):\n",
    "    \"\"\"\n",
    "    Entfernt Outlier aus dem DataFrame, gruppiert nach einer nominalen Spalte.\n",
    "    Die Outlier werden mithilfe der IQR-Methode innerhalb jeder Gruppe identifiziert.\n",
    "    Anschließend wird der DataFrame bereinigt und die Gruppierungsspalte entfernt.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Ursprünglicher DataFrame.\n",
    "        column_name (str): Name der Spalte, in der Outlier erkannt werden sollen.\n",
    "        group_col (str): Name der Spalte, die die nominalen Werte (z.B. 38, 40, 42) enthält.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Bereinigter DataFrame ohne Outlier und ohne die Gruppierungsspalte.\n",
    "    \"\"\"\n",
    "    # Kopie erstellen, um das Original beizubehalten\n",
    "    df_clean = df.copy()\n",
    "    outlier_indices = []\n",
    "\n",
    "    # Gruppierung nach der nominalen Probenhöhe (die zuvor aus den IST-Werten erstellt wurde)\n",
    "    for group_value, group_df in df_clean.groupby(group_col):\n",
    "        Q1 = group_df[column_name].quantile(0.25)\n",
    "        Q3 = group_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Grenzen der IQR-Methode (hier 1.2 * IQR, kann angepasst werden)\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identifikation der Outlier in dieser Gruppe\n",
    "        group_outliers = group_df[(group_df[column_name] < lower_bound) | (group_df[column_name] > upper_bound)]\n",
    "        outlier_indices.extend(group_outliers.index.tolist())\n",
    "\n",
    "    # Doppelte Indizes entfernen\n",
    "    outlier_indices = list(set(outlier_indices))\n",
    "    \n",
    "    # Entferne die Zeilen mit Outliern aus dem DataFrame\n",
    "    df_clean = df_clean.drop(index=outlier_indices)\n",
    "    \n",
    "    # Entferne die nominale Spalte, die zur Gruppierung genutzt wurde\n",
    "    df_clean = df_clean.drop(columns=[group_col])\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def apply_smogn_with_inverse_scaling_and_fallback(\n",
    "    df, target_col, mode_ranges, samples_per_mode=500, k=5,\n",
    "    rel_thres=0.8, random_state=42, min_k=3\n",
    "):\n",
    "    df = df.copy()\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # --- Kategorische Spalten erkennen und label-encoden ---\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "    label_encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # --- Fehlende Werte behandeln ---\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[df.columns] = imputer.fit_transform(df)\n",
    "\n",
    "    # --- Skalierung ---\n",
    "    scaler = StandardScaler()\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    df_scaled = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    all_augmented = []\n",
    "\n",
    "    for min_val, max_val in mode_ranges:\n",
    "        subset = df_scaled[(df_scaled[target_col] >= min_val) & (df_scaled[target_col] <= max_val)].copy()\n",
    "        print(f\"[DEBUG] Bereich {min_val}-{max_val}: subset.shape = {subset.shape}, k = {k}\")\n",
    "\n",
    "        if subset.shape[0] < min_k + 1:\n",
    "            print(f\"[WARN] Zu wenige Daten in Bereich {min_val}-{max_val}, überspringe...\")\n",
    "            continue\n",
    "\n",
    "        subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        rel_method = \"manual\"\n",
    "        rel_ctrl_pts_rg = [\n",
    "            [min_val - 20, 0.0],\n",
    "            [min_val + (max_val - min_val) / 2, 1.0],\n",
    "            [max_val + 20, 0.0]\n",
    "        ]\n",
    "\n",
    "        success = False\n",
    "        current_k = k\n",
    "        while not success and current_k >= min_k:\n",
    "            try:\n",
    "                smogn_result = smogn.smoter(\n",
    "                    data=subset.copy(),\n",
    "                    y=target_col,\n",
    "                    samp_method=\"balance\",\n",
    "                    rel_method=rel_method,\n",
    "                    rel_ctrl_pts_rg=rel_ctrl_pts_rg,\n",
    "                    rel_thres=rel_thres,\n",
    "                    k=current_k\n",
    "                )\n",
    "                new_rows = smogn_result.iloc[subset.shape[0]:].copy()\n",
    "                if new_rows.empty:\n",
    "                    raise ValueError(\"Keine neuen Daten erzeugt durch SMOGN.\")\n",
    "                if new_rows.shape[0] > samples_per_mode:\n",
    "                    new_rows = new_rows.sample(n=samples_per_mode, random_state=random_state)\n",
    "                new_rows[\"synthetisch\"] = 1\n",
    "                all_augmented.append(new_rows)\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"[FALLBACK] SMOGN fehlgeschlagen für Bereich {min_val}-{max_val}. Fehler: {e}\")\n",
    "                current_k -= 1\n",
    "\n",
    "        if not success:\n",
    "            try:\n",
    "                jittered = subset.sample(n=samples_per_mode, replace=True, random_state=random_state)\n",
    "                jitter_cols = jittered.select_dtypes(include=np.number).columns.drop(target_col)\n",
    "                jittered[jitter_cols] += np.random.normal(0, 0.01, size=jittered[jitter_cols].shape)\n",
    "                jittered[\"synthetisch\"] = 1\n",
    "                all_augmented.append(jittered)\n",
    "                print(f\"[FALLBACK] Jitter erfolgreich für Bereich {min_val}-{max_val}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Auch Jitter fehlgeschlagen für Bereich {min_val}-{max_val}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_augmented:\n",
    "        raise ValueError(\"Keine synthetischen Daten erzeugt.\")\n",
    "\n",
    "    df_augmented_scaled = pd.concat(all_augmented, ignore_index=True)\n",
    "    X_new = df_augmented_scaled.drop(columns=[target_col, \"synthetisch\"])\n",
    "    X_new_inverse = pd.DataFrame(scaler.inverse_transform(X_new), columns=X.columns)\n",
    "    df_augmented = pd.concat([\n",
    "        X_new_inverse,\n",
    "        df_augmented_scaled[[target_col, \"synthetisch\"]].reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        le = label_encoders[col]\n",
    "        df_augmented[col] = df_augmented[col].round().astype(int).clip(0, len(le.classes_) - 1)\n",
    "        df_augmented[col] = le.inverse_transform(df_augmented[col])\n",
    "\n",
    "    df_original = df.copy()\n",
    "    df_original[\"synthetisch\"] = 0\n",
    "\n",
    "    df_final = pd.concat([df_original, df_augmented], ignore_index=True)\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Spezifische Anwendung für Bauteil Temp**\n",
    "target_column = \"Bauteil_Temp\"\n",
    "\n",
    "# Datensatz laden\n",
    "df = pd.read_pickle(\"../datasets/new_features.pkl\")\n",
    "# Definiere die nominalen Werte\n",
    "\n",
    "nominal_values = np.array([800,1200]) # Ungefähre Temperauren in original Daten\n",
    "\n",
    "# Funktion, um den nächstgelegenen nominalen Wert zu bestimmen\n",
    "def assign_nominal_value(ist_value, nominal_values=nominal_values):\n",
    "    distances = np.abs(nominal_values - ist_value)\n",
    "    return nominal_values[np.argmin(distances)]\n",
    "\n",
    "# Hilfssplte \n",
    "df['NominalBauteil_Temp'] = df['Bauteil_Temp'].apply(assign_nominal_value)\n",
    "print(\"Originale Daten:\")\n",
    "outlier_df1 = detect_and_visualize_outliers_by_group(df, \"Bauteil_Temp\",group_col=\"NominalBauteil_Temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen von Outliern \n",
    "df_clean = remove_outliers_by_group(df, column_name=\"Bauteil_Temp\", group_col=\"NominalBauteil_Temp\")\n",
    "\n",
    "# Split in Training und Test\n",
    "train_df, test_df = train_test_split(df_clean, test_size=test_size, random_state=42)\n",
    "train_df.to_pickle(f\"../datasets/train_{target_column}_clean.pkl\")\n",
    "test_df.to_pickle(f\"../datasets/test_{target_column}_clean.pkl\")\n",
    "irrelevant_columns = ['Material_con', 'Position_con', 'Ergebnis_con', 'richtig_verbaut', 'Zeit', 'VersuchID','umformzeit','Probenhoehe',\n",
    "                      ]\n",
    "df_reduced = df_clean.drop(columns=irrelevant_columns, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_ranges = [(730, 780),(780,830),(830,880),(1080,1180) ,(1180, 1250)]\n",
    "df_augmented = apply_smogn_with_inverse_scaling_and_fallback(\n",
    "    df_reduced,\n",
    "    target_col=\"Bauteil_Temp\",\n",
    "    mode_ranges=mode_ranges,\n",
    "    samples_per_mode=1000  # optional mehr Samples pro Bereich\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfssplte \n",
    "df_augmented['NominalBauteil_Temp'] = df_augmented['Bauteil_Temp'].apply(assign_nominal_value)\n",
    "\n",
    "outlier_df_test_jittered = detect_and_visualize_outliers_by_group(df_augmented, \"Bauteil_Temp\",group_col=\"NominalBauteil_Temp\")\n",
    "clean_train_df = remove_outliers_by_group(df_augmented, column_name=\"Bauteil_Temp\", group_col=\"NominalBauteil_Temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfssplte \n",
    "clean_train_df['NominalBauteil_Temp'] = clean_train_df['Bauteil_Temp'].apply(assign_nominal_value)\n",
    "\n",
    "outlier_df_test_jittered = detect_and_visualize_outliers_by_group(clean_train_df, \"Bauteil_Temp\",group_col=\"NominalBauteil_Temp\")\n",
    "\n",
    "# Entfernen NominalBauteil_Temp\n",
    "clean_train_df = clean_train_df.drop(columns=['NominalBauteil_Temp',], errors='ignore')\n",
    "\n",
    "# Wieder gruppieren\n",
    "clean_train_df['Bauteil_Temp_Gruppe'] = clean_train_df['Bauteil_Temp'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annahme: Die Features sind alle Spalten außer der Zielvariable & kategorischen Spalten\n",
    "features = [col for col in clean_train_df.columns if col not in ['Ergebnis_con', 'VersuchID', 'Material_con', 'Position_con', 'richtig_verbaut','Probenhoehe']]\n",
    "\n",
    "\n",
    "# Boxplots getrennt nach Bauteil_Temp\n",
    "# for feature in features:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='synthetisch', y=feature, data=clean_train_df, palette=\"Set2\")\n",
    "    \n",
    "#     plt.title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "#     plt.xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "#     plt.ylabel(feature)\n",
    "#     plt.legend(title=\"Bauteil_Temp\", loc=\"upper right\")\n",
    "    \n",
    "#     plt.show()\n",
    "\n",
    "# Setze die Features für die Subplots\n",
    "features_to_plot = [\"auftreffposition\", \"Verkippung_2_Min\"]\n",
    "\n",
    "# Erstelle die Subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # 1 Zeile, 2 Spalten\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    sns.boxplot(x='synthetisch', y=feature, data=clean_train_df, palette=\"Set2\", ax=axes[i])\n",
    "    axes[i].set_title(f'Vergleich der Verteilung für {feature} nach Klassen')\n",
    "    axes[i].set_xlabel(\"Synthetisch (True = künstliche Datenpunkte, False = reale Datenpunkte)\")\n",
    "    axes[i].set_ylabel(feature)\n",
    "\n",
    "# Subplots platzsparend anordnen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung Verteilung der Daten: original vs synthetisch\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.histplot(df_clean['Bauteil_Temp'], bins=300, kde=True, color=\"#4C72B0\", ax=axs[0])\n",
    "axs[0].set_title(\"Verteilung der Bauteiltemperatur im bereinigten originalen Datensatz\")\n",
    "axs[0].set_xlabel(\"Bauteiltemperatur (°C)\")\n",
    "axs[0].set_ylabel(\"Häufigkeit\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "sns.histplot(df_augmented['Bauteil_Temp'], bins=300, kde=True, color=\"#4C72B0\", ax=axs[1])\n",
    "axs[1].set_title(\"Verteilung der Bauteiltemperatur im augmentierten Datensatz\")\n",
    "axs[1].set_xlabel(\"Bauteiltemperatur (°C)\")\n",
    "axs[1].set_ylabel(\"Häufigkeit\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"BauteilTempVerteilungSMOGN.svg\", format=\"svg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausreißer entfernen \n",
    "df_train = clean_train_df.drop(columns=['synthetisch','Bauteil_Temp_Gruppe'], errors='ignore')\n",
    "#df_train.to_pickle(f\"../datasets/train_{target_column}_erweitert.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bauteil_Temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Spezifische Anwendung für Bauteil Temp**\n",
    "target_column = \"Bauteil_Temp\"\n",
    "\n",
    "# Datensatz laden\n",
    "df = pd.read_pickle(\"../datasets/new_features.pkl\")\n",
    "\n",
    "# Split in Training und Test\n",
    "train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "\n",
    "# Erweiterung der Trainingsdaten\n",
    "extended_train_df = extend_regression_target_with_jitter(train_df, target_column, new_sample_count)\n",
    "\n",
    "if test_size == 0.3:\n",
    "        train_path = f\"../Datasets/train_{target_column}_smote_30.pkl\"\n",
    "        test_path = f\"../Datasets/test_{target_column}_30.pkl\"\n",
    "else:\n",
    "        train_path = f\"../Datasets/train_{target_column}_smote.pkl\"\n",
    "        test_path = f\"../Datasets/test_{target_column}.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der neuen Daten\n",
    "# extended_train_df.to_pickle(train_path)\n",
    "# test_df.to_pickle(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
