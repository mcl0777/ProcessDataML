{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellierung der Vorhersage der Probenhöhe\n",
    "\n",
    "\n",
    "Dieses Notebook beschäftigt sich mit der Entwicklung von Vorhersagemodellen für die Zielgröße. Hierzu werden verschiedene Featuresets und Modelle verwendet, um die bestmögliche Vorhersagegenauigkeit zu erreichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstellungen und Datenvorbereitung\n",
    "\n",
    "- **Import relevanter Bibliotheken**\n",
    "- **Metrik:** Die Leistung der Modelle wird anhand der `neg_mean_absolute_error`-Metriken bewertet.\n",
    "- **Zielgröße (Target):** `Probenhoehe`\n",
    "- **Irrelevante Spalten:** Diese Spalten werden aus dem Feature-Set entfernt, da sie für die Vorhersage nicht benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from IPython.display import display, HTML\n",
    "from helpers.model_utils import (load_and_prepare_data, \n",
    "                                 display_top_models_for_target, \n",
    "                                 train_and_tune_models_regression, \n",
    "                                 compare_results_regression,  \n",
    "                                 save_scores_to_csv, \n",
    "                                 analyze_model_performance, \n",
    "                                 plot_scores_and_percent_diff,\n",
    "                                 direcetion_results,\n",
    "                                 compare_smote_effects,\n",
    "                                 filter_and_retrain_with_vif)\n",
    "from helpers.model_pipelines import  (define_pipelines_Probenhoehe, \n",
    "                                      shap_analysis, \n",
    "                                      load_and_split_data,\n",
    "                                      shap_analysis_single_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen Notebook\n",
    "target_name = \"Probenhoehe\"\n",
    "\n",
    "# Metrik auswählen\n",
    "metric = \"neg_mean_absolute_error\"  \n",
    "\n",
    "# Zielgröße\n",
    "target_column = 'Probenhoehe'  \n",
    "irrelevant_columns = ['Material_con', 'Position_con', 'Ergebnis_con', 'richtig_verbaut', 'Zeit', 'VersuchID','umformzeit','synthetisch'\n",
    "                      ]\n",
    "\n",
    "# Einfluss der Verkippungssensoren\n",
    "verkippungsfeatures = [\"Verkippung_1\", \"Verkippung_2\", \"Verkippung_3\", \"Verkippung_4\", \n",
    "                      \"Verkippung_1_Min\", \"Verkippung_1_Max\", \"Verkippung_1_Mean\", \"Verkippung_1_Median\", \"Verkippung_1_Std\", \n",
    "                      \"Verkippung_2_Min\",\"Verkippung_2_Max\",\"Verkippung_2_Mean\",\"Verkippung_2_Median\",\"Verkippung_2_Std\",\"Verkippung_3_Min\",\n",
    "                      \"Verkippung_3_Max\",\"Verkippung_3_Mean\",\"Verkippung_3_Median\",\"Verkippung_3_Std\",\"Verkippung_4_Min\",\"Verkippung_4_Max\",\n",
    "                      \"Verkippung_4_Mean\",\"Verkippung_4_Median\",\"Verkippung_4_Std\", \"tilt_x_tiefster\", \"tilt_y_tiefster\", \"tilt_x_t0\", \"tilt_y_t0\"]\n",
    "\n",
    "# Mapping Pfad für Labels\n",
    "label_mapping_path = \"../mappings/label_mappings_binary.json\"\n",
    "\n",
    "# Verkippung berücksichtigen?\n",
    "Verkippung = True\n",
    "\n",
    "if not Verkippung:\n",
    "    irrelevant_columns.extend(verkippungsfeatures)\n",
    "    \n",
    "# SHAP Analyse durchführen?\n",
    "shap_on = False\n",
    "\n",
    "plot_type = 'summary'  # Alternativ: \"summary\", \"bar\" oder \"interaction\" oder \"violin\"\n",
    "plot_type = str(plot_type)\n",
    "\n",
    "\n",
    "# Plots speichern?\n",
    "save_plots = True\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# mit SMOTE (synthethische Erweiterung der Trainingsdaten)?\n",
    "smote_on = True\n",
    "\n",
    "# Bautiel Temperatur berücksichtigen?\n",
    "bauteil_temp = True\n",
    "if not bauteil_temp:\n",
    "    irrelevant_columns.append('Bauteil_Temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu den Daten\n",
    "test_filepath = f\"../datasets/test_{target_column}.pkl\"\n",
    "if smote_on:\n",
    "    # Pfade zu den SMOTE Trainingsdaten\n",
    "    train_filepath = f\"../datasets/train_{target_column}_smogn.pkl\"\n",
    "    test_filepath = f\"../datasets/test_{target_column}_clean.pkl\"\n",
    "else: \n",
    "    save_plots = False\n",
    "    train_filepath = f\"../datasets/train_{target_column}_clean.pkl\"\n",
    "    test_filepath = f\"../datasets/test_{target_column}_clean.pkl\"\n",
    "\n",
    "#test_filepath = f\"../datasets/test_{target_column}_jittered.pkl\"\n",
    "\n",
    "# Lade und bereite Trainings- und Testdaten vor\n",
    "X_train_original, y_train = load_and_prepare_data(train_filepath, target_column, irrelevant_columns)\n",
    "X_test_original, y_test = load_and_prepare_data(test_filepath, target_column, irrelevant_columns)\n",
    "    \n",
    "# Überprüfen der Shapes\n",
    "print(f\"Trainingsdaten: X_train={X_train_original.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Testdaten: X_test={X_test_original.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# Zentrale Pipeline und Parameter\n",
    "input_dim = X_train_original.shape[1]\n",
    "target_pipeline = define_pipelines_Probenhoehe()\n",
    "pipelines, param_grids = target_pipeline\n",
    "\n",
    "# Cross-Validation Setup\n",
    "# KFold für Regression\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.round(2)\n",
    "y_test = y_test.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Features Dataset\n",
    "\n",
    "In diesem Abschnitt wird das Datenset für die Prozessmerkmale vorbereitet und trainiert. Das Dataset wird angepasst, indem irrelevante Spalten entfernt und die relevanten Features extrahiert werden. Anschließend werden verschiedene Machine-Learning-Modelle trainiert, die darauf abzielen, die Zielgröße basierend auf den Prozessmerkmalen vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen Dataset\n",
    "dataset_name = \"process_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "\n",
    "# Auswahl der Process Features aus dem Dataset\n",
    "columns_process_features = [\n",
    "    \"VersuchID\",\n",
    "    \"Berührzeit\",\n",
    "    \"Höhe_Wegmessung_Aufprall\",\n",
    "    \"Umformzeit\",\n",
    "    \"Tiefster_Punkt\",\n",
    "    \"Energie_Aufprall\",\n",
    "    \"Motorstrom_Durchschnitt\",\n",
    "    \"Energie_ab_Durchschnitt\",\n",
    "    \"Bauteil_Temp\",\n",
    "    \"Werkzeug_Temp\",\n",
    "    \"Material_con\",\n",
    "    \"Position_con\",\n",
    "    \"Ergebnis_con\",\n",
    "    \"Probenhoehe\",\n",
    "]\n",
    "\n",
    "# Sicherstellen, dass nur existierende Spalten verwendet werden\n",
    "columns_to_keep_train = [col for col in columns_process_features if col in X_train_original.columns]\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original[columns_to_keep_train]\n",
    "X_test = X_test_original[columns_to_keep_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In diesem Abschnitt wird das Training der Modelle durchgeführt. Die Modelle werden mit Hilfe von Cross-Validation optimiert, um eine robuste Bewertung der Modellleistung sicherzustellen. Hyperparameter-Tuning wird genutzt, um die besten Einstellungen für jedes Modell zu finden und die Generalisierungsfähigkeit auf Testdaten zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_Probenhoehe(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_1 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_1.to_html()))\n",
    "#results_df_1.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "save_scores_to_csv(\n",
    "    results=results_df_1,  # Direkt das DataFrame übergeben\n",
    "    output_dir=target_dir,    # Zielverzeichnis\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Dateiname\n",
    "    Verkippung=Verkippung    # Flag für Verkippung\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, model_name=\"Modell\"):\n",
    "    \"\"\"\n",
    "    Erstellt einen Scatter-Plot für tatsächliche vs. vorhergesagte Werte.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Tatsächliche Werte.\n",
    "        y_pred (array-like): Vorhergesagte Werte.\n",
    "        model_name (str): Name des Modells für den Titel.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Streudiagramm\n",
    "    plt.scatter(y_pred, y_true, color='red', alpha=0.6, label=\"Datenpunkte\")\n",
    "    \n",
    "    # Diagonale Linie (perfekte Vorhersage)\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', label=\"Perfekte Vorhersage\")\n",
    "    \n",
    "    # Achsenbeschriftung\n",
    "    plt.xlabel(\"Vorhergesagte Probenhöhe (mm)\")\n",
    "    plt.ylabel(\"Tatsächliche Probenhöhe (mm)\")\n",
    "    plt.title(f\"Tatsächliche vs. vorhergesagte Probenhöhe ({model_name})\")\n",
    "    #plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test\n",
    "\n",
    "In diesem Abschnitt wird die Überprüfung von Overfitting durch die Analyse von Learning Curves und Validation Curves durchgeführt, um den Bias-Variance-Tradeoff besser zu verstehen. Die `analyze_model_performance` Funktion aus der `model_pipelines.py` führt diese Analysen durch und erstellt entsprechende Plots. Es werden sowohl die Trainingsscores als auch die Testscores über verschiedene Größen des Trainingssets und Hyperparameterbereiche betrachtet, um die Modellstabilität und die Generalisierungsfähigkeit zu bewerten. Dies hilft dabei, die optimale Komplexität der Modelle zu bestimmen und sicherzustellen, dass sie gut auf unbekannte Daten generalisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Validierungsverfahren\n",
    "#print(\"\\nAlternative Validierungsverfahren:\")\n",
    "#for name, pipeline in best_pipelines.items():\n",
    "#    print(f\"Alternative Validierungsverfahren für {name}...\")\n",
    "#    test_alternative_validation(pipeline, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse\n",
    "\n",
    "In diesem Abschnitt führen wir eine SHAP-Analyse durch, um die Einflüsse der einzelnen Features auf die Vorhersagen der besten Modelle zu verstehen. Die Ergebnisse dieser Analyse helfen uns, die Modellentscheidungen transparenter zu machen. Anschließend laden wir Diagramme, die die besten drei Modelle basierend auf unterschiedlichen Metriken darstellen. Diese Visualisierungen unterstützen die Bewertung der Modellperformance und die Identifizierung von Schlüsselfeatures, die die Zielgröße beeinflussen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramme aus der Zielgröße laden\n",
    "display_top_models_for_target(\n",
    "    target_dir,\n",
    "    results_df_1,\n",
    "    metric= 'Test MSE',\n",
    "    top_n=3,  # Nur die besten 3 Modelle anzeigen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregierte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen\n",
    "dataset_name = \"aggregated_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "\n",
    "\n",
    "# Auswahl der Process Features aus dem Dataset\n",
    "columns_aggregated_features = [\n",
    "    \"VersuchID\",\n",
    "    \"Berührzeit\",\n",
    "    \"Höhe_Wegmessung_Aufprall\",\n",
    "    \"Umformzeit\",\n",
    "    \"Tiefster_Punkt\",\n",
    "    \"Energie_Aufprall\",\n",
    "    \"Motorstrom_Durchschnitt\",\n",
    "    \"Energie_ab_Durchschnitt\",\n",
    "    \"Bauteil_Temp\",\n",
    "    \"Werkzeug_Temp\",\n",
    "    \"Material_con\",\n",
    "    \"Position_con\",\n",
    "    \"Ergebnis_con\",\n",
    "    \"Probenhoehe\",\n",
    "    \"richtig_verbaut\",\n",
    "    \"Wegmessung_Min\",\n",
    "    \"Wegmessung_Max\",\n",
    "    \"Wegmessung_Mean\",\n",
    "    \"Wegmessung_Median\",\n",
    "    \"Wegmessung_Std\",\n",
    "    \"Verkippung_1_Min\",\n",
    "    \"Verkippung_1_Max\",\n",
    "    \"Verkippung_1_Mean\",\n",
    "    \"Verkippung_1_Median\",\n",
    "    \"Verkippung_1_Std\",\n",
    "    \"Verkippung_2_Min\",\n",
    "    \"Verkippung_2_Max\",\n",
    "    \"Verkippung_2_Mean\",\n",
    "    \"Verkippung_2_Median\",\n",
    "    \"Verkippung_2_Std\",\n",
    "    \"Verkippung_3_Min\",\n",
    "    \"Verkippung_3_Max\",\n",
    "    \"Verkippung_3_Mean\",\n",
    "    \"Verkippung_3_Median\",\n",
    "    \"Verkippung_3_Std\",\n",
    "    \"Verkippung_4_Min\",\n",
    "    \"Verkippung_4_Max\",\n",
    "    \"Verkippung_4_Mean\",\n",
    "    \"Verkippung_4_Median\",\n",
    "    \"Verkippung_4_Std\",\n",
    "    \"Stoesselhub_Min\",\n",
    "    \"Stoesselhub_Max\",\n",
    "    \"Stoesselhub_Mean\",\n",
    "    \"Stoesselhub_Median\",\n",
    "    \"Stoesselhub_Std\",\n",
    "    \"Geschwindigkeit_Ges_Min\",\n",
    "    \"Geschwindigkeit_Ges_Max\",\n",
    "    \"Geschwindigkeit_Ges_Mean\",\n",
    "    \"Geschwindigkeit_Ges_Median\",\n",
    "    \"Geschwindigkeit_Ges_Std\",\n",
    "    \"Presskraft_dyn_Min\",\n",
    "    \"Presskraft_dyn_Max\",\n",
    "    \"Presskraft_dyn_Mean\",\n",
    "    \"Presskraft_dyn_Median\",\n",
    "    \"Presskraft_dyn_Std\",\n",
    "    \"Motorstrom_Min\",\n",
    "    \"Motorstrom_Max\",\n",
    "    \"Motorstrom_Mean\",\n",
    "    \"Motorstrom_Median\",\n",
    "    \"Motorstrom_Std\",\n",
    "]\n",
    "\n",
    "# Sicherstellen, dass nur existierende Spalten verwendet werden\n",
    "columns_to_keep_train = [col for col in columns_aggregated_features if col in X_train_original.columns]\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original[columns_to_keep_train]\n",
    "X_test = X_test_original[columns_to_keep_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_Probenhoehe(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_2 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_2.to_html()))\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=results_df_2,  # DataFrame mit den Scores\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Gemeinsamer Basisname\n",
    "    Verkippung=Verkippung  # Fügt den '_no_tilt'-Suffix hinzu, falls nötig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Diagramme aus der Zielgröße \"Prozessqualität\" laden\n",
    "display_top_models_for_target(\n",
    "    target_dir,\n",
    "    results_df_2,\n",
    "    metric = 'Test MSE',\n",
    "    top_n=3  # Nur die besten 3 Modelle anzeigen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen\n",
    "dataset_name = \"new_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original\n",
    "X_test = X_test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original.drop(columns='synthetisch', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_Probenhoehe(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_3 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_3.to_html()))\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=results_df_3,  # DataFrame mit den Scores\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Gemeinsamer Basisname\n",
    "    Verkippung=Verkippung  # Fügt den '_no_tilt'-Suffix hinzu, falls nötig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich der Results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dfs = {\n",
    "    \"Process Features\": results_df_1,\n",
    "    \"Aggregated Features\": results_df_2,\n",
    "    \"New Features\": results_df_3\n",
    "}\n",
    "\n",
    "# Vergleich der Ergebnisse über die Datasets hinweg\n",
    "compare_metric = \"Test MSE\"\n",
    "comparison_table, best_combination = compare_results_regression(results_dfs, metric=compare_metric)\n",
    "\n",
    "# Vergleichstabelle in HTML-Format konvertieren und anzeigen\n",
    "print(f\"Vergleich der {compare_metric}-Scores für die unterschiedlichen Datasets und Modelle:\")\n",
    "display(HTML(comparison_table.to_html()))\n",
    "\n",
    "# Beste Kombination ausgeben\n",
    "print(f\"\\nBeste Kombination:\")\n",
    "print(f\"Dataset: {best_combination[0]}, Modell: {best_combination[1]}, Score: {best_combination[2]:.4f}\")\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=comparison_table,\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"compare_model_scores_{target_name}_{compare_metric}.csv\",\n",
    "    Verkippung=Verkippung\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_f1_scores_and_percent_diff_MSE(results_dfs, target_name, output_path=None):\n",
    "#     \"\"\"\n",
    "#     Erstellt ein Balkendiagramm für den MSE über verschiedene Feature-Sätze und ergänzt\n",
    "#     eine Liniengrafik für die prozentuale Abweichung (percent_diff). Optional wird das Diagramm gespeichert.\n",
    "    \n",
    "#     Args:\n",
    "#         results_dfs (dict): Dictionary mit den DataFrames für verschiedene Feature-Sätze.\n",
    "#         target_name (str): Name des Zielwerts (Target), der im Diagrammtitel verwendet wird.\n",
    "#         output_path (str): Pfad, unter dem das Diagramm gespeichert werden soll (optional).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Modelle aus einem der DataFrames extrahieren\n",
    "#     models = results_dfs[\"Process Features\"].columns\n",
    "\n",
    "#     # MSE für verschiedene Feature-Sätze abrufen\n",
    "#     MSE_process = results_dfs[\"Process Features\"].loc[\"Test MSE\"].values\n",
    "#     MSE_aggregated = results_dfs[\"Aggregated Features\"].loc[\"Test MSE\"].values\n",
    "#     MSE_new = results_dfs[\"New Features\"].loc[\"Test MSE\"].values\n",
    "\n",
    "#     # Prozentuale Abweichung (percent_diff_MSE) abrufen\n",
    "#     percent_diff_process = results_dfs[\"Process Features\"].loc[\"percent_diff_MSE\"].values\n",
    "#     percent_diff_aggregated = results_dfs[\"Aggregated Features\"].loc[\"percent_diff_MSE\"].values\n",
    "#     percent_diff_new = results_dfs[\"New Features\"].loc[\"percent_diff_MSE\"].values\n",
    "\n",
    "#     # Breite der Balken und Positionen auf der X-Achse\n",
    "#     bar_width = 0.2\n",
    "#     x = np.arange(len(models))\n",
    "\n",
    "#     # Erstellen des Balkendiagramms für MSE\n",
    "#     fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "#     ax1.bar(x - bar_width, MSE_process, width=bar_width, label=\"Prozess Features\", color='firebrick')\n",
    "#     ax1.bar(x, MSE_aggregated, width=bar_width, label=\"Aggregierte Features\", color='goldenrod')\n",
    "#     ax1.bar(x + bar_width, MSE_new, width=bar_width, label=\"Neue Features\", color='forestgreen')\n",
    "    \n",
    "#     # Achsenbeschriftung und Titel\n",
    "#     ax1.set_xlabel(\"Modelle\")\n",
    "#     ax1.set_ylabel(\"MSE\", color='black')\n",
    "#     ax1.set_title(\"Vergleich der Modellleistung für die {} mit unterschiedlichen Feature-Datensätzen\".format(target_name))\n",
    "#     ax1.set_xticks(x)\n",
    "#     ax1.set_xticklabels(models, rotation=45)\n",
    "#     ax1.legend(loc=\"upper left\")\n",
    "\n",
    "#     # Skalierung der linken Y-Achse anpassen\n",
    "#     ax1.set_ylim(0, 1)  # MSE zwischen 0 und 1\n",
    "\n",
    "#     # Zweite Y-Achse für percent_diff\n",
    "#     ax2 = ax1.twinx()\n",
    "#     ax2.plot(x, percent_diff_process, marker='o', linestyle='-', color='maroon', label=\"Prozentuale Differenz (Prozess)\")\n",
    "#     ax2.plot(x, percent_diff_aggregated, marker='s', linestyle='-', color='darkgoldenrod', label=\"Prozentuale Differenz (Aggregiert)\")\n",
    "#     ax2.plot(x, percent_diff_new, marker='^', linestyle='-', color='darkgreen', label=\"Prozentuale Differenz (Neu)\")\n",
    "\n",
    "#     # Skalierung der rechten Y-Achse anpassen\n",
    "#     max_percent_diff = max(\n",
    "#         np.max(percent_diff_process),\n",
    "#         np.max(percent_diff_aggregated),\n",
    "#         np.max(percent_diff_new)\n",
    "#     )\n",
    "#     ax2.set_ylim(0, max_percent_diff * 1.2)  # Dynamische Skalierung mit etwas Puffer\n",
    "\n",
    "#     ax2.set_ylabel(\"%-Abweichung zwischen Train- & Testdatensatz\", color='black')\n",
    "#     ax2.legend(loc=\"upper right\")\n",
    "\n",
    "#     # Diagramm speichern, falls ein Pfad angegeben ist\n",
    "#     if output_path:\n",
    "#         # Verzeichnis erstellen, falls es nicht existiert\n",
    "#         os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#         plt.savefig(output_path, dpi=300, bbox_inches='tight')  # Speichert das Diagramm mit hoher Auflösung\n",
    "\n",
    "#     # Diagramm anzeigen\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"../results/{target_name}/{balance_suffix}{tilt_suffix}{temp_suffix}/compare_model_scores_{target_name}_{compare_metric}{balance_suffix}{tilt_suffix}{temp_suffix}.svg\"\n",
    "\n",
    "# Diagramm erstellen\n",
    "# RMSE macht mehr Sinn prozentual zu vergleichen als der MSE, da dieser in Original Einheiten ist\n",
    "plot_scores_and_percent_diff(results_dfs, target_name=target_name, output_path=output_path, y_lim=(0, 0.4), metric=\"Test MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduktion der Features & Performance des Best Models\n",
    "- Das beste Modell wird nach der Feature Reduktion mit Hilfe der SHAP Values im Zielverzeichis gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell und Feature Datensatz auswählen\n",
    "model_name = \"XGBoost\"\n",
    "pipeline = best_pipelines[model_name]\n",
    "dataset_name = \"new_features\"\n",
    "\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp, model_name=model_name)\n",
    "\n",
    "# Verzeichnis erstellen, falls es nicht existiert\n",
    "os.makedirs(target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_path_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse für ein einzelnes Modell\n",
    "shap_analysis_single_model(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    X_train=X_train,\n",
    "    target_name=target_name,\n",
    "    dataset_name=dataset_name,\n",
    "    output_dir=target_dir_best,\n",
    "    plot_type=plot_type,\n",
    "    save_plots=save_plots,\n",
    "    verkippung=Verkippung\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **AUFRUF DER FUNKTION**\n",
    "best_pipelines, retrain_results = filter_and_retrain_with_vif(\n",
    "    model_name=model_name,\n",
    "    pipeline=pipeline,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    feature_importance_path=feature_importance_path_best,\n",
    "    param_grids=param_grids,\n",
    "    cv=kf,\n",
    "    scoring=metric,\n",
    "    pareto_split=0.8,  \n",
    "    correlation_threshold=0.9,  \n",
    "    vif_threshold=75.0,  \n",
    "    output_dir=target_dir_best,\n",
    "    target_name=target_name,\n",
    "    dataset_name=dataset_name,\n",
    "    verkippung=Verkippung,\n",
    "    is_regression=True,\n",
    "    label_mapping_path=label_mapping_path,    # Neuer Parameter für Confusion-Matrix\n",
    "    target_column=target_column          # Neuer Parameter für Confusion-Matri  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierungen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, model_name=\"Modell\"):\n",
    "    \"\"\"\n",
    "    Erstellt einen Scatter-Plot für tatsächliche vs. vorhergesagte Werte.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Tatsächliche Werte.\n",
    "        y_pred (array-like): Vorhergesagte Werte.\n",
    "        model_name (str): Name des Modells für den Titel.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Streudiagramm\n",
    "    plt.scatter(y_pred, y_true, color='red', alpha=0.6, label=\"Datenpunkte\")\n",
    "    \n",
    "    # Diagonale Linie (perfekte Vorhersage)\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', label=\"Perfekte Vorhersage\")\n",
    "    \n",
    "    # Achsenbeschriftung\n",
    "    plt.xlabel(\"Vorhergesagte Probenhöhe (mm)\")\n",
    "    plt.ylabel(\"Tatsächliche Probenhöhe (mm)\")\n",
    "    plt.title(f\"Tatsächliche vs. vorhergesagte Probenhöhe ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE für Prozess Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_path = f\"../results/{target_name}/{tilt_suffix}{temp_suffix}/process_features/model_scores_process_features.csv\"\n",
    "no_smote_path = f\"../results/{target_name}/noSMOTE/process_features/model_scores_process_featuresnoSMOTE.csv\"\n",
    "output_path = f\"../results/{target_name}/compare_SMOTE_process_features_{target_name}_{compare_metric}.svg\"\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "df_comp = compare_smote_effects(no_smote_path, smote_path, metric=\"Test MSE\", title=f\"Vergleich der Fehlermetrik für das Prozess Feature-Set: No SMOGN vs. SMOGN\", save_path=output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
