{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellierung der Vorhersage der Bauteiltemperatur\n",
    "\n",
    "\n",
    "Dieses Notebook beschäftigt sich mit der Entwicklung von Vorhersagemodellen für die Zielgröße. Hierzu werden verschiedene Featuresets und Modelle verwendet, um die bestmögliche Vorhersagegenauigkeit zu erreichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einstellungen und Datenvorbereitung\n",
    "\n",
    "- **Import relevanter Bibliotheken**\n",
    "- **Metrik:** Die Leistung der Modelle wird anhand der `neg_mean_absolute_error`-Metriken bewertet.\n",
    "- **Zielgröße (Target):** `Bauteil_Temp`\n",
    "- **Irrelevante Spalten:** Diese Spalten werden aus dem Feature-Set entfernt, da sie für die Vorhersage nicht benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from IPython.display import display, HTML\n",
    "from helpers.model_utils import (load_and_prepare_data, \n",
    "                                 display_top_models_for_target, \n",
    "                                 train_and_tune_models_regression, \n",
    "                                 compare_results_regression,  \n",
    "                                 save_scores_to_csv, \n",
    "                                 analyze_model_performance, \n",
    "                                 plot_scores_and_percent_diff,\n",
    "                                 direcetion_results,\n",
    "                                 compare_smote_effects,\n",
    "                                 filter_and_retrain_with_vif)\n",
    "from helpers.model_pipelines import  (define_pipelines_BauteilTemp, \n",
    "                                      shap_analysis, \n",
    "                                      load_and_split_data,\n",
    "                                      shap_analysis_single_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen Notebook\n",
    "target_name = \"Bauteiltempartur\"\n",
    "\n",
    "# Metrik auswählen\n",
    "metric = \"neg_mean_absolute_error\"  \n",
    "\n",
    "# Zielgröße\n",
    "target_column = 'Bauteil_Temp'  \n",
    "irrelevant_columns = ['Material_con', 'Position_con', 'Ergebnis_con', 'richtig_verbaut', 'Zeit', 'VersuchID','umformzeit','synthetisch','Probenhoehe'\n",
    "                      ]\n",
    "\n",
    "# Einfluss der Verkippungssensoren\n",
    "verkippungsfeatures = [\"Verkippung_1\", \"Verkippung_2\", \"Verkippung_3\", \"Verkippung_4\", \n",
    "                      \"Verkippung_1_Min\", \"Verkippung_1_Max\", \"Verkippung_1_Mean\", \"Verkippung_1_Median\", \"Verkippung_1_Std\", \n",
    "                      \"Verkippung_2_Min\",\"Verkippung_2_Max\",\"Verkippung_2_Mean\",\"Verkippung_2_Median\",\"Verkippung_2_Std\",\"Verkippung_3_Min\",\n",
    "                      \"Verkippung_3_Max\",\"Verkippung_3_Mean\",\"Verkippung_3_Median\",\"Verkippung_3_Std\",\"Verkippung_4_Min\",\"Verkippung_4_Max\",\n",
    "                      \"Verkippung_4_Mean\",\"Verkippung_4_Median\",\"Verkippung_4_Std\", \"tilt_x_tiefster\", \"tilt_y_tiefster\", \"tilt_x_t0\", \"tilt_y_t0\"]\n",
    "\n",
    "# Mapping Pfad für Labels\n",
    "label_mapping_path = \"../data_preparation/mappings/label_mappings_binary.json\"\n",
    "\n",
    "# Verkippung berücksichtigen?\n",
    "Verkippung = True\n",
    "\n",
    "if not Verkippung:\n",
    "    irrelevant_columns.extend(verkippungsfeatures)\n",
    "    \n",
    "# SHAP Analyse durchführen?\n",
    "shap_on = False\n",
    "\n",
    "plot_type = 'summary'  # Alternativ: \"summary\", \"bar\" oder \"interaction\" oder \"violin\"\n",
    "plot_type = str(plot_type)\n",
    "\n",
    "\n",
    "# Plots speichern?\n",
    "save_plots = True\n",
    "\n",
    "# mit SMOTE (synthethische Erweiterung der Trainingsdaten)?\n",
    "smote_on = True\n",
    "\n",
    "# Bauteil Temp\n",
    "bauteil_temp = True # Es gibt keine Bauteiltemperatur, True nur für die Verzeichniserstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade zu den Daten\n",
    "test_filepath = f\"../datasets/test_{target_column}.pkl\"\n",
    "if smote_on:\n",
    "    # Pfade zu den SMOTE Trainingsdaten\n",
    "    train_filepath = f\"../datasets/train_{target_column}_erweitert.pkl\"\n",
    "    test_filepath = f\"../datasets/test_{target_column}_clean.pkl\"\n",
    "else: \n",
    "    save_plots = False\n",
    "    train_filepath = f\"../datasets/train_{target_column}_clean.pkl\"\n",
    "    test_filepath = f\"../datasets/test_{target_column}_clean.pkl\"\n",
    "\n",
    "# Lade und bereite Trainings- und Testdaten vor\n",
    "X_train_original, y_train = load_and_prepare_data(train_filepath, target_column, irrelevant_columns)\n",
    "X_test_original, y_test = load_and_prepare_data(test_filepath, target_column, irrelevant_columns)\n",
    "    \n",
    "# Überprüfen der Shapes\n",
    "print(f\"Trainingsdaten: X_train={X_train_original.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Testdaten: X_test={X_test_original.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# Zentrale Pipeline und Parameter\n",
    "input_dim = X_train_original.shape[1]\n",
    "target_pipeline = define_pipelines_BauteilTemp()\n",
    "pipelines, param_grids = target_pipeline\n",
    "\n",
    "# Cross-Validation Setup\n",
    "# KFold für Regression\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.round(2)\n",
    "y_test = y_test.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Features Dataset\n",
    "\n",
    "In diesem Abschnitt wird das Datenset für die Prozessmerkmale vorbereitet und trainiert. Das Dataset wird angepasst, indem irrelevante Spalten entfernt und die relevanten Features extrahiert werden. Anschließend werden verschiedene Machine-Learning-Modelle trainiert, die darauf abzielen, die Zielgröße basierend auf den Prozessmerkmalen vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen Dataset\n",
    "dataset_name = \"process_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "\n",
    "# Auswahl der Process Features aus dem Dataset\n",
    "columns_process_features = [\n",
    "    \"VersuchID\",\n",
    "    \"Berührzeit\",\n",
    "    \"Höhe_Wegmessung_Aufprall\",\n",
    "    \"Umformzeit\",\n",
    "    \"Tiefster_Punkt\",\n",
    "    \"Energie_Aufprall\",\n",
    "    \"Motorstrom_Durchschnitt\",\n",
    "    \"Energie_ab_Durchschnitt\",\n",
    "    #\"Bauteil_Temp\",\n",
    "    \"Werkzeug_Temp\",\n",
    "    \"Material_con\",\n",
    "    \"Position_con\",\n",
    "    \"Ergebnis_con\",\n",
    "    \"Probenhoehe\",\n",
    "]\n",
    "\n",
    "# Sicherstellen, dass nur existierende Spalten verwendet werden\n",
    "columns_to_keep_train = [col for col in columns_process_features if col in X_train_original.columns]\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original[columns_to_keep_train]\n",
    "X_test = X_test_original[columns_to_keep_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In diesem Abschnitt wird das Training der Modelle durchgeführt. Die Modelle werden mit Hilfe von Cross-Validation optimiert, um eine robuste Bewertung der Modellleistung sicherzustellen. Hyperparameter-Tuning wird genutzt, um die besten Einstellungen für jedes Modell zu finden und die Generalisierungsfähigkeit auf Testdaten zu verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_BauteilTemp(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_1 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_1.to_html()))\n",
    "#results_df_1.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "save_scores_to_csv(\n",
    "    results=results_df_1,  # Direkt das DataFrame übergeben\n",
    "    output_dir=target_dir,    # Zielverzeichnis\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Dateiname\n",
    "    Verkippung=Verkippung    # Flag für Verkippung\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, model_name=\"Modell\"):\n",
    "    \"\"\"\n",
    "    Erstellt einen Scatter-Plot für tatsächliche vs. vorhergesagte Werte.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Tatsächliche Werte.\n",
    "        y_pred (array-like): Vorhergesagte Werte.\n",
    "        model_name (str): Name des Modells für den Titel.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Streudiagramm\n",
    "    plt.scatter(y_pred, y_true, color='red', alpha=0.6, label=\"Datenpunkte\")\n",
    "    \n",
    "    # Diagonale Linie (perfekte Vorhersage)\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', label=\"Perfekte Vorhersage\")\n",
    "    \n",
    "    # Achsenbeschriftung\n",
    "    plt.xlabel(\"Vorhergesagte Bauteiltemperatur (° C)\")\n",
    "    plt.ylabel(\"Tatsächliche Bauteiltemperatur (° C)\")\n",
    "    plt.title(f\"Tatsächliche vs. vorhergesagte Bauteiltemperatur ({model_name})\")\n",
    "    #plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test\n",
    "\n",
    "In diesem Abschnitt wird die Überprüfung von Overfitting durch die Analyse von Learning Curves und Validation Curves durchgeführt, um den Bias-Variance-Tradeoff besser zu verstehen. Die `analyze_model_performance` Funktion aus der `model_pipelines.py` führt diese Analysen durch und erstellt entsprechende Plots. Es werden sowohl die Trainingsscores als auch die Testscores über verschiedene Größen des Trainingssets und Hyperparameterbereiche betrachtet, um die Modellstabilität und die Generalisierungsfähigkeit zu bewerten. Dies hilft dabei, die optimale Komplexität der Modelle zu bestimmen und sicherzustellen, dass sie gut auf unbekannte Daten generalisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Validierungsverfahren\n",
    "#print(\"\\nAlternative Validierungsverfahren:\")\n",
    "#for name, pipeline in best_pipelines.items():\n",
    "#    print(f\"Alternative Validierungsverfahren für {name}...\")\n",
    "#    test_alternative_validation(pipeline, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse\n",
    "\n",
    "In diesem Abschnitt führen wir eine SHAP-Analyse durch, um die Einflüsse der einzelnen Features auf die Vorhersagen der besten Modelle zu verstehen. Die Ergebnisse dieser Analyse helfen uns, die Modellentscheidungen transparenter zu machen. Anschließend laden wir Diagramme, die die besten drei Modelle basierend auf unterschiedlichen Metriken darstellen. Diese Visualisierungen unterstützen die Bewertung der Modellperformance und die Identifizierung von Schlüsselfeatures, die die Zielgröße beeinflussen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagramme aus der Zielgröße laden\n",
    "display_top_models_for_target(\n",
    "    target_dir,\n",
    "    results_df_1,\n",
    "    metric= 'Test MSE',\n",
    "    top_n=3,  # Nur die besten 3 Modelle anzeigen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregierte Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen\n",
    "dataset_name = \"aggregated_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "\n",
    "\n",
    "# Auswahl der Process Features aus dem Dataset\n",
    "columns_aggregated_features = [\n",
    "    \"VersuchID\",\n",
    "    \"Berührzeit\",\n",
    "    \"Höhe_Wegmessung_Aufprall\",\n",
    "    \"Umformzeit\",\n",
    "    \"Tiefster_Punkt\",\n",
    "    \"Energie_Aufprall\",\n",
    "    \"Motorstrom_Durchschnitt\",\n",
    "    \"Energie_ab_Durchschnitt\",\n",
    "    #\"Bauteil_Temp\",\n",
    "    \"Werkzeug_Temp\",\n",
    "    \"Material_con\",\n",
    "    \"Position_con\",\n",
    "    \"Ergebnis_con\",\n",
    "    \"Probenhoehe\",\n",
    "    \"richtig_verbaut\",\n",
    "    \"Wegmessung_Min\",\n",
    "    \"Wegmessung_Max\",\n",
    "    \"Wegmessung_Mean\",\n",
    "    \"Wegmessung_Median\",\n",
    "    \"Wegmessung_Std\",\n",
    "    \"Verkippung_1_Min\",\n",
    "    \"Verkippung_1_Max\",\n",
    "    \"Verkippung_1_Mean\",\n",
    "    \"Verkippung_1_Median\",\n",
    "    \"Verkippung_1_Std\",\n",
    "    \"Verkippung_2_Min\",\n",
    "    \"Verkippung_2_Max\",\n",
    "    \"Verkippung_2_Mean\",\n",
    "    \"Verkippung_2_Median\",\n",
    "    \"Verkippung_2_Std\",\n",
    "    \"Verkippung_3_Min\",\n",
    "    \"Verkippung_3_Max\",\n",
    "    \"Verkippung_3_Mean\",\n",
    "    \"Verkippung_3_Median\",\n",
    "    \"Verkippung_3_Std\",\n",
    "    \"Verkippung_4_Min\",\n",
    "    \"Verkippung_4_Max\",\n",
    "    \"Verkippung_4_Mean\",\n",
    "    \"Verkippung_4_Median\",\n",
    "    \"Verkippung_4_Std\",\n",
    "    \"Stoesselhub_Min\",\n",
    "    \"Stoesselhub_Max\",\n",
    "    \"Stoesselhub_Mean\",\n",
    "    \"Stoesselhub_Median\",\n",
    "    \"Stoesselhub_Std\",\n",
    "    \"Geschwindigkeit_Ges_Min\",\n",
    "    \"Geschwindigkeit_Ges_Max\",\n",
    "    \"Geschwindigkeit_Ges_Mean\",\n",
    "    \"Geschwindigkeit_Ges_Median\",\n",
    "    \"Geschwindigkeit_Ges_Std\",\n",
    "    \"Presskraft_dyn_Min\",\n",
    "    \"Presskraft_dyn_Max\",\n",
    "    \"Presskraft_dyn_Mean\",\n",
    "    \"Presskraft_dyn_Median\",\n",
    "    \"Presskraft_dyn_Std\",\n",
    "    \"Motorstrom_Min\",\n",
    "    \"Motorstrom_Max\",\n",
    "    \"Motorstrom_Mean\",\n",
    "    \"Motorstrom_Median\",\n",
    "    \"Motorstrom_Std\",\n",
    "]\n",
    "\n",
    "# Sicherstellen, dass nur existierende Spalten verwendet werden\n",
    "columns_to_keep_train = [col for col in columns_aggregated_features if col in X_train_original.columns]\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original[columns_to_keep_train]\n",
    "X_test = X_test_original[columns_to_keep_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_BauteilTemp(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_2 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_2.to_html()))\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=results_df_2,  # DataFrame mit den Scores\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Gemeinsamer Basisname\n",
    "    Verkippung=Verkippung  # Fügt den '_no_tilt'-Suffix hinzu, falls nötig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Diagramme aus der Zielgröße \"Prozessqualität\" laden\n",
    "display_top_models_for_target(\n",
    "    target_dir,\n",
    "    results_df_2,\n",
    "    metric = 'Test MSE',\n",
    "    top_n=3  # Nur die besten 3 Modelle anzeigen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einstellungen\n",
    "dataset_name = \"new_features\"\n",
    "target_dir, feature_importance_path, balance_suffix, feature_importance_path_best, target_dir_best, tilt_suffix, temp_suffix = direcetion_results(dataset_name, target_name, smote_on, Verkippung, bauteil_temp)\n",
    "\n",
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original\n",
    "X_test = X_test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train und X_test auf relevante Spalten beschränken\n",
    "X_train = X_train_original.drop(columns='synthetisch', errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining und Bewertung\n",
    "input_dim = X_train.shape[1]\n",
    "pipelines ,param_grids= define_pipelines_BauteilTemp(input_dim)\n",
    "best_pipelines, results = train_and_tune_models_regression(\n",
    "    pipelines, \n",
    "    param_grids, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    kf, \n",
    "    X_test=X_test, \n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "# Ergebnisse in DataFrame speichern\n",
    "results_df_3 = pd.DataFrame.from_dict(results, orient='index').T\n",
    "#results_df.drop(['best_params'], axis=0, inplace=True)\n",
    "\n",
    "# Ergebnisse in tabellarischer Form anzeigen\n",
    "display(HTML(results_df_3.to_html()))\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=results_df_3,  # DataFrame mit den Scores\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"model_scores_{dataset_name}{balance_suffix}.csv\",  # Gemeinsamer Basisname\n",
    "    Verkippung=Verkippung  # Fügt den '_no_tilt'-Suffix hinzu, falls nötig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "# Schritt 2: Vorhersagen für ein Modell generieren (z. B. XGBoost)\n",
    "for model_name in best_pipelines:\n",
    "    model = best_pipelines[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Schritt 3: Visualisierung\n",
    "    plot_actual_vs_predicted(y_test, y_pred, model_name=model_name)\n",
    "else:\n",
    "    print(f\"Fehler: Modell '{model_name}' wurde nicht gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting überprüfen \n",
    "# Analyse von Learning Curves für Bias-Variance-Tradeoff\n",
    "print(f\"\\nLearning Curve Analyse für {target_name}:\")\n",
    "\n",
    "# Analyse der Modellleistung\n",
    "analyze_model_performance(\n",
    "    pipelines=pipelines, \n",
    "    param_grids=param_grids, \n",
    "    X=X_train, \n",
    "    y=y_train, \n",
    "    scoring=metric, \n",
    "    cv=kf,\n",
    "    save_plots=save_plots,\n",
    "    output_dir= target_dir,\n",
    "    Verkippung=Verkippung\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-Analyse\n",
    "if shap_on:\n",
    "    shap_analysis(\n",
    "        best_pipelines=best_pipelines,\n",
    "        X_train=X_train,\n",
    "        target_name=target_name,\n",
    "        dataset_name=dataset_name,\n",
    "        output_dir=target_dir,\n",
    "        plot_type=plot_type,  # Alternativ: \"bar\" oder \"interaction\" oder \"violin\"\n",
    "        save_plots=save_plots,\n",
    "        verkippung=Verkippung\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich der Results \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dfs = {\n",
    "    \"Process Features\": results_df_1,\n",
    "    \"Aggregated Features\": results_df_2,\n",
    "    \"New Features\": results_df_3\n",
    "}\n",
    "\n",
    "# Vergleich der Ergebnisse über die Datasets hinweg\n",
    "compare_metric = \"Test MSE\"\n",
    "comparison_table, best_combination = compare_results_regression(results_dfs, metric=compare_metric)\n",
    "\n",
    "# Vergleichstabelle in HTML-Format konvertieren und anzeigen\n",
    "print(f\"Vergleich der {compare_metric}-Scores für die unterschiedlichen Datasets und Modelle:\")\n",
    "display(HTML(comparison_table.to_html()))\n",
    "\n",
    "# Beste Kombination ausgeben\n",
    "print(f\"\\nBeste Kombination:\")\n",
    "print(f\"Dataset: {best_combination[0]}, Modell: {best_combination[1]}, Score: {best_combination[2]:.4f}\")\n",
    "\n",
    "save_scores_to_csv(\n",
    "    results=comparison_table,\n",
    "    output_dir=target_dir,\n",
    "    file_name=f\"compare_model_scores_{target_name}_{compare_metric}.csv\",\n",
    "    Verkippung=Verkippung\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"../results/{target_name}/{balance_suffix}{tilt_suffix}{temp_suffix}/compare_model_scores_{target_name}_{compare_metric}{balance_suffix}{tilt_suffix}{temp_suffix}.svg\"\n",
    "\n",
    "# Diagramm erstellen\n",
    "# RMSE macht mehr Sinn prozentual zu vergleichen als der MSE, da dieser in Original Einheiten ist\n",
    "plot_scores_and_percent_diff(results_dfs, target_name=target_name, output_path=output_path, y_lim=(0, 50), metric=\"Test MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisierungen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_actual_vs_predicted(y_true, y_pred, model_name=\"Modell\"):\n",
    "    \"\"\"\n",
    "    Erstellt einen Scatter-Plot für tatsächliche vs. vorhergesagte Werte.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Tatsächliche Werte.\n",
    "        y_pred (array-like): Vorhergesagte Werte.\n",
    "        model_name (str): Name des Modells für den Titel.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Streudiagramm\n",
    "    plt.scatter(y_pred, y_true, color='red', alpha=0.6, label=\"Datenpunkte\")\n",
    "    \n",
    "    # Diagonale Linie (perfekte Vorhersage)\n",
    "    min_val = min(min(y_true), min(y_pred))\n",
    "    max_val = max(max(y_true), max(y_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', label=\"Perfekte Vorhersage\")\n",
    "    \n",
    "    # Achsenbeschriftung\n",
    "    plt.xlabel(\"Vorhergesagte Probenhöhe (mm)\")\n",
    "    plt.ylabel(\"Tatsächliche Probenhöhe (mm)\")\n",
    "    plt.title(f\"Tatsächliche vs. vorhergesagte Probenhöhe ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE für Prozess Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_path = f\"../results/{target_name}/{tilt_suffix}{temp_suffix}/process_features/model_scores_process_features.csv\"\n",
    "no_smote_path = f\"../results/{target_name}/noSMOTE{tilt_suffix}{temp_suffix}/process_features/model_scores_process_featuresnoSMOTE.csv\"\n",
    "output_path = f\"../results/{target_name}/compare_SMOTE_process_features_{target_name}_{compare_metric}.svg\"\n",
    "df_comp = compare_smote_effects(no_smote_path, smote_path, metric=\"Test MAE\", title=f\"Vergleich der Fehlermetrik für das Prozess Feature-Set: No SMOGN vs. SMOGN\", save_path=output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (conda310)",
   "language": "python",
   "name": "conda310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
